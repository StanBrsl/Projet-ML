{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e542a6",
   "metadata": {},
   "source": [
    "# Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7182811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path : C:/Users/Stanislas Brusselle/OneDrive/Documents/Projet ML sklearn/des-fichiers-complementaires-3-049-ko/Telechargement/data/iris.csv\n",
      "path : C:/Users/Stanislas Brusselle/OneDrive/Documents/Projet ML sklearn/des-fichiers-complementaires-3-049-ko/Telechargement/data/titanic_train.csv\n",
      "path : C:/Users/Stanislas Brusselle/OneDrive/Documents/Projet ML sklearn/des-fichiers-complementaires-3-049-ko/Telechargement/data/boston.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog as fd\n",
    "\n",
    "def select_file_csv():\n",
    "    \n",
    "    init = \"C:\\\\Users\\\\Stanislas Brusselle\\\\OneDrive\\\\Documents\\\\Projet ML sklearn\\\\des-fichiers-complementaires-3-049-ko\\\\Telechargement\\\\data\"\n",
    "    \n",
    "    file_type = (('csv file', '*.csv'), ('All file', '*.*'))\n",
    "    file_name = fd.askopenfilename(initialdir = init, title='Fichiers', filetypes = file_type)\n",
    "    \n",
    "    return file_name\n",
    "\n",
    "def select_file_txt():\n",
    "    \n",
    "    init = \"C:\\\\Users\\\\Stanislas Brusselle\\\\OneDrive\\\\Documents\\\\Projet ML sklearn\\\\des-fichiers-complementaires-3-049-ko\\\\Telechargement\\\\data\"\n",
    "    \n",
    "    file_type = (('csv file', '*.txt'), ('All file', '*.*'))\n",
    "    file_name = fd.askopenfilename(initialdir = init, title='Fichiers', filetypes = file_type)\n",
    "    \n",
    "    return file_name\n",
    "\n",
    "def cvs_to_dataFrame():\n",
    "    \n",
    "    root=tk.Tk()  \n",
    "    root.overrideredirect(1)\n",
    "\n",
    "    path=select_file_csv()\n",
    "\n",
    "    root.destroy()\n",
    "    \n",
    "    print(\"path : \" + path)\n",
    "    \n",
    "    with open(path, mode='r') as file : \n",
    "        fichier = pd.read_csv(file)\n",
    "        \n",
    "    return fichier\n",
    "\n",
    "def txt_to_dataFrame(nom_colonnes, skip_rows):\n",
    "    \n",
    "    root=tk.Tk()  \n",
    "    root.overrideredirect(1)\n",
    "\n",
    "    path=select_file_txt()\n",
    "\n",
    "    root.destroy()\n",
    "    \n",
    "    print(\"path : \" + path)\n",
    "    \n",
    "    with open(path, mode='r') as file : \n",
    "        fichier = pd.read_fwf(file, skip_rows=skip_rows, header=None, names=nom_colonnes)\n",
    "\n",
    "    return fichier\n",
    "\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "\n",
    "iris_df = cvs_to_dataFrame()\n",
    "titanic_df = cvs_to_dataFrame()\n",
    "boston_df = txt_to_dataFrame(names, 22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c45e3",
   "metadata": {},
   "source": [
    "# 5) Préparer les catégories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d580826e",
   "metadata": {},
   "source": [
    "## 5.1) Validation des données "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18233b36",
   "metadata": {},
   "source": [
    "Comme pour les données numériques, les données catégorielles doivent être préparées. Pour la validation, il peut être utile de vérifier combien d'éléments sont attribués à chaque catégories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "460f8a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Iris-setosa        50\n",
       "Iris-versicolor    50\n",
       "Iris-virginica     50\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5dc72d",
   "metadata": {},
   "source": [
    "## 5.2) Modification des catégories "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c58db",
   "metadata": {},
   "source": [
    "Savoir si les catégories sont ordonnées et si non, les ordonner :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20aa80da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_cat(dataset, column, order):\n",
    "    if dataset[column].cat.ordered==False:\n",
    "        dataset[column].cat.reorder_categories(order, ordered=True, inplace=True)\n",
    "        return dataset\n",
    "    else : \n",
    "        return dataset\n",
    "    \n",
    "organize_cat(titanic_df, 'Pclass', [1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d3657f",
   "metadata": {},
   "source": [
    "Remplacer une catégorie par une autre : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe9b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df['title'].replce('Mlle', 'Miss', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f887c045",
   "metadata": {},
   "source": [
    "Supprimer les catégories inutiles : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92d21d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df['title'].cat.remove_unused_categories(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50c3bdd",
   "metadata": {},
   "source": [
    "Fusionner plusieurs catégories : \n",
    "\n",
    "Ici, je regroupe tous les passagers dont le titre est présent moins de 20 fois en une seule catégorie Other. \n",
    "Pour cela, je calcule dans un premier temps le nombre d'occurences, je tri ensuite les catégories peu fréqentes puis les remplace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8971b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_values = titanic_df['title'].value_counts()\n",
    "to_replace = nb_values[nb_values<20].index\n",
    "titanic_df['title']=titanic_df['title'].replace({x:'Other'for x in to_replace}).astype('category')\n",
    "titanic_df['title'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567efa33",
   "metadata": {},
   "source": [
    "## 5.3) Quantification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9cb1e1",
   "metadata": {},
   "source": [
    "Cette étape consiste à remplacer une variable catégorielle en une variable numérique. Il n'est pas judicieux de remplacer une catégorie par un nombre. En effet, une catégorie n'a pas de raison de compter plus qu'une autre selon le nombre qui est attribué. On utilisera donc la méthode de dummification. (=> vectoriser les catégories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bf732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_df = pd.get_dummies(titanic['title'], prefix = 't')#cela créer une nouvelle colonne title_df qu'il concatener au dataset\n",
    "new_df = pd.concat([titanic_df, title_df]), axis= 1)\n",
    "\n",
    "#Je supprime enfin la colonne avec les catégories catégorielles\n",
    "\n",
    "new_df.drop(columns='title', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c615bac",
   "metadata": {},
   "source": [
    "La fonction get_dummies a un paramètre drop_first qui précise s'il faut éliminer une colonne (la première). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b8063e",
   "metadata": {},
   "source": [
    "# 6) Les données particulières : dates et texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f4a7e6",
   "metadata": {},
   "source": [
    "## 6.1) Préparation des dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dc199f",
   "metadata": {},
   "source": [
    "Le format datetime64 de pandas : \n",
    "1) joindre chaque partie d'un date dans la même champs (jour, mois et année séparés par des '-')\n",
    "\n",
    "2) convertir les dates au bon format\n",
    "\n",
    "Si la date est déjà présente dans le dataset, elle sera reconnue au chargement comme un objet(donc une chaine de caractères), il faudra utiliser pd.to_datetime pour la convertir. Le paramètre dayfirst permet de stipuler si la date est format américain ou européen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b795ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour réunir tous les caractères de date en un seul champs. \n",
    "df['date']= df['jour'].astype('str')+'-'+df['mois'].astype('str')+'-'+df['année'].astype('str')\n",
    "#pour transformer cette chaine de caractère en format datetime64\n",
    "df['date']=pd.to_datetime(df['date'], dayfirst=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f00aca7",
   "metadata": {},
   "source": [
    "Il y aussi possibilité d'ajouter l'heure au format datetime64 de la même manière que si dessus (les heure et les minutes doivent être séparés par le caractère ':') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95a2fed",
   "metadata": {},
   "source": [
    "### 6.1.1) Extraire les composants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7056402",
   "metadata": {},
   "source": [
    "Une fois la date au bon format il est possible d'extraire les composants de la date. \n",
    "\n",
    "df['date']=df['date'].dt.nomcomposant\n",
    "\n",
    "nomcomposant : \n",
    "\n",
    "- dayofyear\n",
    "- dayofweek\n",
    "- quarter\n",
    "- week"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9096c03",
   "metadata": {},
   "source": [
    "### 6.1.2) Gérer les écarts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80b1fa0",
   "metadata": {},
   "source": [
    "Il est parfois utile de calculer l'écart entre deux dates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22924381",
   "metadata": {},
   "source": [
    "La libraire permet d'ajouter ou enlever une durée à un date grâce à la fonction pd.Timedelta. Il suffit d'indiquer en paramètres la durée souhaitée (en millisecondes comme en années) et de l'ajouter ou la soustraire à une variable au bon format (datetime64). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01855fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] + pd.Timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b07a39",
   "metadata": {},
   "source": [
    "Il est aussi possible d'ajouter ou soustraire deux dates :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33421ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date limite']= df['date actuelle']+df['date rendue']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341b7d82",
   "metadata": {},
   "source": [
    "## 6.2) Préparer les chaines de caractères "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d25e7f",
   "metadata": {},
   "source": [
    "### 6.2.1) Préparer les chaines "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41794931",
   "metadata": {},
   "source": [
    "=> la mise de tout texte en minuscule\n",
    "\n",
    "=> suppression des espaces à gauches ou à droite du texte\n",
    "\n",
    "=> remplacement de certain caractères comme les accents\n",
    "\n",
    "Tout passe par la méthode str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26034f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#passage en minuscule : \n",
    "titanic_df['Name_lower']=titanic_df['Name'].str.lower\n",
    "#passage en majuscule\n",
    "titanic_df['Name_lower']=titanic_df['Name'].str.upper\n",
    "#supprimer les espaces\n",
    "titanic_df['Name_lower']=titanic_df['Name'].str.strip\n",
    "#supprimer les espaces à gauche\n",
    "titanic_df['Name_lower']=titanic_df['Name'].str.lstrip\n",
    "#supprimer les espaces à droite\n",
    "titanic_df['Name_lower']=titanic_df['Name'].str.rstrip\n",
    "#remplacer un caractère par un autre\n",
    "#supprimer les espaces à gauchec\n",
    "titanic_df['Name_lower']=titanic_df['Name'].str.replace(',', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c482e0",
   "metadata": {},
   "source": [
    "### 6.2.2) Effectuer une recherche dans les chaines  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8e26c7",
   "metadata": {},
   "source": [
    "Une fois la chaine de caractères traitée  il est possible d'effectuer une recherche dans celle-ci.\n",
    "\n",
    "df['caractère']=df['caractère'].str.nomcomposant\n",
    "\n",
    "nomcomposant : \n",
    "\n",
    "- contains : indique si une chaine contient une sous-chaien donnée\n",
    "- count : compte le nombre d'occurence d'une sous-chaine dans une chaine\n",
    "- startswitch / endswitch : indique si une chaine commence ou se termine par une sous-chaine\n",
    "- findall : renvoie toutes les occurences d'une sous-chaine sur tous les enregistrements\n",
    "- match : indique si les valeurs correspondent à un pattern \n",
    "- index : cherche la première occurence d'un sous-chaine et renvoie son index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dd827d",
   "metadata": {},
   "source": [
    "### 6.2.3) Extraire des sous-chaînes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337db907",
   "metadata": {},
   "source": [
    "Il est également courant de devoir extraire une sous chaine d'un chaine de caractères. De la même manière que pour la recherche, on utilisera le fonction str. \n",
    "\n",
    "nomcomposant : \n",
    "\n",
    "- slice : extraire une sous-chaine en indiquant son début et sa fin\n",
    "- extract : extraire une sous-chaine via des expressions régulières (n'extrait que la première correspondance)\n",
    "- extractall : pareil que la fonction extract mais extrait toutes les correspondances\n",
    "- split : sépare une chaine de caractères sur un séparateur ou un sous-chaine régulière\n",
    "- partition : partitionne un chaine de caractères selon un séparateur ou un expression régulière"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc94beb",
   "metadata": {},
   "source": [
    "# 7) Les transfomers : automatiser la préparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d71bafe",
   "metadata": {},
   "source": [
    "Une fois le modèle déployé il faudra lui fournir des données en temps réel ou en mode batch. Mais celle-ci nécessiterons aussi un prétraitement car elles devront être au même format que les données d'entrainement. Il faudra créer en parallèle de la préparation des données une pipeline de préparation : la suite de toutes les étapes nécessaires pour passer d'une entrée brute à une donnée traitée. Non seulement il faut conserver les opérations effectuées mais également les valeurs utilisées (telles que min ou max) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7102157",
   "metadata": {},
   "source": [
    "## 7.1) Pipelines avec Scikit-learn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21518fd7",
   "metadata": {},
   "source": [
    "Je ne détaillerai pas trop cette partie car l'objectif pour moi est de créer mon propre transformer\n",
    "\n",
    "Il existe une succession d'étapes appelées 'Transformer', chainées sur le dataset pour faire de la gestion des pipelines. \n",
    "\n",
    "Création d'un Transformer : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b18b63f",
   "metadata": {},
   "source": [
    "- init : constructeur du Transformer\n",
    "- fit : calculer les paramètres nécessaires \n",
    "- transform "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
